<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>CVD-STORM</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://dorverbin.github.io/refnerf">
    <meta property="og:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta property="og:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <!-- <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>CVD-STORM</b>: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving <br>
                <!-- <small>
                    
                </small> -->
            </h2>
        </div>
        <div class="is-size-4 publication-authors">
              <span style="color: black;" class="author-block">Tianrui Zhang<sup>2</sup>,</span>
              <span style="color: black;" class="author-block">Yichen Liu<sup>1</sup>,</span>
              <span style="color: black;" class="author-block">Zilin Guo<sup>2</sup>,</span>
              <span style="color: black;" class="author-block">Jingcheng Ni<sup>1</sup>,</span>
              <span style="color: black;" class="author-block">Yuxin Guo<sup>1</sup>,</span>
              <span style="color: black;" class="author-block">Chenjing Ding<sup>1</sup>,</span>
              <span style="color: black;" class="author-block">Dan Xu<sup>2</sup>,</span>
              <span style="color: black;" class="author-block">Lewei Lu<sup>1</sup>,</span>
              <span style="color: black;" class="author-block">Zehuan Wu<sup>1</sup>,</span>
        </div>
        <div class="is-size-4 publication-authors">
            <span class="author-block"><sup>1</sup>SenseTime Research</span>
            <span class="author-block"><sup>2</sup>The Hong Kong University of Science and Technology</span>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="http://arxiv.org/abs/2312.01531">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://youtu.be/qrdRH9irAlk">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset</strong></h4>
                            </a>
                        </li>
                        <li>  
                            <a href="https://dorverbin.github.io/refnerf/data/shiny_blender_source.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset Source</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">
                            <image src="img/real_database_icon.png" height="60px">
                                <h4><strong>Real Dataset</strong></h4>
                            </a>
                        </li>                             -->
                        <li>
                            <a href="https://github.com/SenseTime-FVG/OpenDWM" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>
                </div>
			</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose \ours, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.
                </p>
            </div>
        </div>

        <image src="img/arch.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <!-- <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div> -->
                    <video id="demo" width="100%" playsinline controls muted>
                        <source src="video/demo.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Mask Aggregation
                </h3>
                <div class="text-justify">
                    Previous works directly decode masks from distilled SAM features, or use self-prompting to iteratively inverse render SAM masks. 
                    The former suffers from aliasing in the rendered SAM features and cannot produce consistent masks across views, while the latter may accumulate errors due to incorrect self-prompting.
                    Hence, we propose to aggregate the imperfect SAM 2D masks in the 3D space to generate high-quality and consistent 3D masks.                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <!-- <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video> -->
                    <img src="./img/comparison_1.png" width="100%">
                    <img src="./img/comparison_2.png" width="100%">
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Ray-Pair RGB Loss
                </h3>
                <div class="text-justify">
                    Segmentation errors in both 3D and 2D are more likely to occur at object boundaries. One observation is that humans usually distinguish object boundaries by the color and texture difference on the two sides. Here we introduce the Ray-Pair RGB loss, aiming to incorporate color and spatial information to improve the segmentation quality.                
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ray-pair_rgb_loss.png" width="50%">
                    <img src="./img/sim.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    We sample rays from regions with high training error and reproject the 3D surface points to different training views to gather image patches. The Ray-Pair RGB Loss is then applied to a set of reference rays and all other rays in these patches. This allows us to regularize the object field using appearance from multiple views simultaneously. <br><br>
                </div>
                <!-- <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Comparison
                </h3>
                <!-- <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div> -->
                <div class="text-center">
                    <img src="./img/comparison_3.png" width="100%">
                    <img src="./img/comparison_4.png" width="100%">
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Other Prompts
                </h3>
                Our method also support text prompts using Grounding-DINO, and automatically segmenting everything in the scene using a grid of point prompts:
                <!-- <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div> -->
                <div class="text-center">
                    <img src="./img/application.png" width="60%">
                </div>
			</div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{liu2023sanerfhq,
    title={SANeRF-HQ: Segment Anything for NeRF in High Quality},
    author={Liu, Yichen and Hu, Benran and Tang, Chi-Keung and Tai, Yu-Wing},
    journal={arXiv preprint arXiv:2312.01531},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>
                </p>
            </div>
        </div>
    </div>


</body></html>
